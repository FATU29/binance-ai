# TODO List - AI Service Implementation

## ðŸŽ¯ TÃ³m Táº¯t Nhanh

### âœ… ÄÃ£ Xong (CÃ³ thá»ƒ dÃ¹ng ngay)

- [x] Sentiment Analysis vá»›i OpenAI
- [x] News Management (CRUD)
- [x] Database models cho News & Sentiment
- [x] API endpoints cÆ¡ báº£n
- [x] Documentation cho FE integration

### âŒ Cáº§n LÃ m Gáº¥p

- [ ] News Crawler
- [ ] Price History Service
- [ ] Binance API Integration
- [ ] News-Price Alignment
- [ ] Causal Analysis
- [ ] VIP Feature Gating

---

## ðŸ“‹ Chi Tiáº¿t Tasks

### Priority 1: Critical (Cáº§n lÃ m trÆ°á»›c)

#### 1. Implement Price History Service

**Files cáº§n táº¡o/sá»­a**:

- `app/db/models/price_history.py` (hiá»‡n táº¡i: EMPTY)
- `app/services/binance_service.py` (hiá»‡n táº¡i: EMPTY)
- `app/services/price_history.py` (hiá»‡n táº¡i: EMPTY)
- `app/schemas/price_history.py` (chÆ°a táº¡o)
- `app/api/v1/endpoints/price_history.py` (chÆ°a táº¡o)

**Tasks**:

```bash
1. Táº¡o PriceHistory model trong database
2. Viáº¿t service fetch data tá»« Binance API
3. Táº¡o endpoint GET /api/v1/prices/{symbol}
4. Táº¡o background task Ä‘á»ƒ sync price Ä‘á»‹nh ká»³
5. Test vá»›i BTCUSDT vÃ  ETHUSDT
```

**Estimate**: 1-2 ngÃ y

---

#### 2. Implement Basic News Crawler

**Files cáº§n táº¡o**:

- `app/services/crawler_service.py` (chÆ°a cÃ³)
- `app/services/html_parser.py` (chÆ°a cÃ³)
- `app/core/crawler_config.py` (chÆ°a cÃ³)
- `app/api/v1/endpoints/crawler.py` (chÆ°a cÃ³)

**Tasks**:

```bash
1. Install dependencies: beautifulsoup4, feedparser
2. Táº¡o RSS feed parser cho CoinDesk, CoinTelegraph
3. Táº¡o HTML scraper cÆ¡ báº£n
4. Implement scheduled crawling (APScheduler)
5. Test crawler vá»›i 2-3 nguá»“n
```

**Estimate**: 2-3 ngÃ y

---

### Priority 2: Important (LÃ m tiáº¿p sau)

#### 3. Implement News-Price Alignment

**Files cáº§n sá»­a**:

- `app/services/alignment_service.py` (hiá»‡n táº¡i: EMPTY)
- `app/schemas/alignment.py` (chÆ°a táº¡o)
- `app/api/v1/endpoints/alignment.py` (chÆ°a táº¡o)
- `app/db/models/alignment.py` (chÆ°a táº¡o - optional)

**Tasks**:

```bash
1. Viáº¿t logic align news time vá»›i price time
2. TÃ­nh correlation giá»¯a sentiment vÃ  price change
3. Táº¡o API endpoint Ä‘á»ƒ FE query alignment
4. Prepare features cho ML models (náº¿u cáº§n)
```

**Estimate**: 2-3 ngÃ y

---

#### 4. Implement VIP Feature Gating

**Files cáº§n sá»­a**:

- `app/core/security.py` (cÃ³ sáºµn nhÆ°ng minimal)
- `app/core/dependencies.py` (cáº§n thÃªm role checking)
- Táº¥t cáº£ endpoints cáº§n restrict (add VIP dependency)

**Tasks**:

```bash
1. Implement JWT token verification
2. Táº¡o UserRole enum (Regular, VIP, Admin)
3. Táº¡o require_vip() dependency
4. Apply vÃ o cÃ¡c endpoints advanced (causal analysis, etc.)
5. Test vá»›i mock tokens
```

**Estimate**: 1 ngÃ y

---

### Priority 3: Advanced (LÃ m sau cÃ¹ng)

#### 5. Implement Causal Analysis

**Files cáº§n táº¡o**:

- `app/services/causal_analysis_service.py` (chÆ°a cÃ³)
- `app/schemas/causal_analysis.py` (chÆ°a cÃ³)
- `app/api/v1/endpoints/causal.py` (chÆ°a cÃ³)

**Tasks**:

```bash
1. Design prompt template cho OpenAI
2. Implement explain_price_movement()
3. Táº¡o API endpoint POST /ai/causal/explain
4. Add caching cho causal explanations
5. Test vá»›i real price movements
```

**Estimate**: 2-3 ngÃ y

---

#### 6. Intelligent Crawler Enhancement

**Files cáº§n sá»­a**:

- `app/services/crawler_service.py`
- `app/services/html_parser.py`

**Tasks**:

```bash
1. Implement structure detection algorithm
2. Add adaptive parsing khi website thay Ä‘á»•i
3. Add error handling & retry logic
4. Implement rate limiting
5. Add monitoring & logging
```

**Estimate**: 3-4 ngÃ y

---

## ðŸ”§ Quick Fixes (CÃ³ thá»ƒ lÃ m ngay)

### Fix 1: Add OpenAI API Key Check

**File**: `app/core/config.py`

```python
# Add validation
@validator("OPENAI_API_KEY")
def validate_openai_key(cls, v):
    if not v or v == "your_openai_api_key_here":
        logger.warning("âš ï¸ OpenAI API key not set! Sentiment analysis will use fallback.")
    return v
```

---

### Fix 2: Add Health Check for OpenAI

**File**: `app/api/v1/endpoints/health.py`

```python
@router.get("/detailed")
async def detailed_health():
    return {
        "status": "healthy",
        "openai_available": bool(settings.OPENAI_API_KEY),
        "database_connected": True,  # Check DB connection
        "features": {
            "sentiment_analysis": True,
            "news_management": True,
            "price_history": False,
            "news_crawler": False,
            "alignment": False,
            "causal_analysis": False
        }
    }
```

---

### Fix 3: Add CORS for Frontend

**File**: `main.py`

```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## ðŸ“ Database Migrations Needed

### Migration 1: Add price_history table

```bash
cd ai
uv run alembic revision --autogenerate -m "Add price_history table"
uv run alembic upgrade head
```

### Migration 2: Add alignment table (optional)

```bash
uv run alembic revision --autogenerate -m "Add news_price_alignment table"
uv run alembic upgrade head
```

### Migration 3: Add causal_analysis table (optional)

```bash
uv run alembic revision --autogenerate -m "Add causal_analysis table"
uv run alembic upgrade head
```

---

## ðŸ§ª Testing Checklist

### Current Features (Test Now)

- [ ] POST /news - Create article
- [ ] GET /news - List articles
- [ ] GET /news/{id} - Get single article
- [ ] POST /ai/analyze/quick - Quick sentiment
- [ ] POST /ai/analyze/news/{id} - Analyze article
- [ ] GET /ai/news/{id}/latest - Get sentiment

### Future Features (Test Later)

- [ ] POST /crawler/run - Manual crawl trigger
- [ ] GET /prices/{symbol} - Get price history
- [ ] POST /alignment/analyze - Alignment analysis
- [ ] POST /causal/explain - Causal explanation

---

## ðŸ“¦ Dependencies to Add

**Add to `pyproject.toml`**:

```toml
[project]
dependencies = [
    # ... existing ...

    # Web Scraping
    "beautifulsoup4>=4.12.0",
    "lxml>=5.0.0",
    "feedparser>=6.0.0",
    "playwright>=1.40.0",  # For JS-heavy sites

    # Background Tasks
    "apscheduler>=3.10.0",

    # Data Processing
    "pandas>=2.1.0",
    "numpy>=1.26.0",
]
```

**Install**:

```bash
cd ai
uv sync
```

---

## ðŸš€ Deployment Checklist

### Before Deploy:

- [ ] Set OPENAI_API_KEY in production .env
- [ ] Run database migrations
- [ ] Test all endpoints
- [ ] Add CORS for production FE URL
- [ ] Set up logging
- [ ] Add error monitoring (Sentry?)

### Docker Setup:

- [ ] Update dockerfile if needed
- [ ] Update docker-compose.yml
- [ ] Test Docker build
- [ ] Test Docker run

---

## ðŸ“ž Support

**When stuck, check**:

- API Docs: http://localhost:8000/docs
- FE Integration Guide: `FE_INTEGRATION_GUIDE.md`
- Status Summary: `AI_STATUS_SUMMARY.md`
- Requirements: `REQUIREMENTS_ANALYSIS.md`

**Contact**: Team Lead / Project Manager

---

**Last Updated**: December 25, 2025
**Priority**: Start with Priority 1 tasks!
